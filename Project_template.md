# Задание 1. Исследование моделей и инфраструктуры

## 1. Сравнение LLM-моделей (локальные Hugging Face vs облачные OpenAI / YandexGPT)  


### Mistral-7B-Instruct-v0.1 (локальная)

- Качество ответов: 
	- Хорошо справляется с задачами генерации текста и ответами на вопросы. Может требовать более тщательного промт-инжиниринга для достижения оптимальных результатов.

- Скорость работы:
	- Относительно быстрая, хорошо подходит для интерактивных приложений. Может быть ускорена с помощью квантизации и оптимизации.

- Стоимость владения и использования:
	- Бесплатная для использования (Apache 2.0 license). Затраты на оборудование и электроэнергию для локального запуска.

- Удобство и простота развёртывания:
	- Можно развернуть с помощью Hugging Face Transformers. Доступны готовые примеры и документация. Поддерживается сообществом.


### Meta-Llama-3-8B-Instruct (локальная)

- Качество ответов:
	- Хорошо понимает контекст и генерирует релевантные ответы.

- Скорость работы:
	- Достаточно высокая скорость работы. 

- Стоимость владения и использования:
	- Бесплатная для использования (Llama 3 Community License Agreement). Затраты на оборудование и электроэнергию для локального запуска.

- Удобство и простота развёртывания:
	- Можно развернуть с помощью Hugging Face Transformers. Требует установки дополнительных зависимостей (например, torch, transformers). Поддерживается сообществом.


### GPT-4 OpenAI (облачная)

- Качество ответов:
	- Превосходное качество ответов. Отличное понимание контекста, сложные рассуждения, генерация креативного контента. Требует минимального промт-инжиниринга.

- Скорость работы:
	- Зависит от загруженности API

- Стоимость владения и использования:
	- Оплата за токен.  Есть бесплатный лимит.

- Удобство и простота развёртывания:
	- Доступ через API, требует регистрации. Просто интегрируется с помощью библиотек Python (например, openai).


### YandexGPT (облачная)

- Качество ответов:
	- Неплохое качество ответов. Хорошо подходит для генерации текста на русском языке.

- Скорость работы:
	- Скорость также зависит от загруженности API. 

- Стоимость владения и использования:
	- Оплата за токен

- Удобство и простота развёртывания:
	- Доступ через API Yandex Просто интегрируется с помощью библиотек Python


### Конфиденциальность

В случае с локальными моделями полный контроль над данными. В случае с облачными вариантами данные могут передаваться в облако, что может быть неприемлемо. 



## 2. Сравнение моделей эмбеддингов (локальные Sentence-Transformers vs облачные OpenAI Embeddings)


### all-MiniLM-L6-v2 (локальная Sentence-Transformers)

- Скорость создания индекса: 
	- Очень высокая. Модель относительно небольшая, что позволяет быстро обрабатывать большие объемы текста.

- Качество поиска: 
	- Хорошее, особенно для задач семантического поиска и кластеризации. all-MiniLM-L6-v2 - это хороший компромисс между скоростью и точностью.

- Стоимость владения и использования: 
	- Бесплатная (Apache 2.0 license). Затраты на оборудование и электроэнергию для локального запуска.


### text-embedding-ada-002 (облачная OpenAI Embeddings)

- Скорость создания индекса: 
	- Зависит от загруженности API OpenAI. Может быть медленнее локальных моделей при больших объемах данных.

- Качество поиска: 
	- Отличное. Обеспечивает высокую точность поиска за счет большого размера модели и обучения на огромном количестве данных.

- Стоимость владения и использования: 
	- Оплата за токен. Зависит от объема данных и частоты использования.
                                               

## 3. Сравнение векторных баз ChromaDB и FAISS:


### ChromaDB

- Скорость создания индекса:
	- Зависит от объема данных и аппаратного обеспечения. Может быть медленнее, чем FAISS, особенно для больших наборов данных. 

- Качество поиска:
	- Хорошее. ChromaDB использует алгоритмы approximate nearest neighbor search (ANN) и поддерживает фильтрацию по метаданным для улучшения точности.

- Стоимость владения и использования:
	- Зависит от способа развертывания (локально или в облаке). Локальное развертывание требует затрат на оборудование. Облачные сервисы ChromaDB взимают плату за хранение и использование.


### FAISS

- Скорость создания индекса:
	- Высокая. FAISS оптимизирован для быстрой индексации, особенно при использовании GPU. Поддерживает различные методы индексации для оптимизации скорости и точности.

- Качество поиска:
	- Отличное. FAISS предоставляет различные алгоритмы ANN, позволяющие настраивать баланс между скоростью и точностью поиска. Поддерживает квантование векторов и другие методы сжатия для уменьшения объема памяти и повышения скорости поиска.

- Стоимость владения и использования:
	- Бесплатно. FAISS - это библиотека с открытым исходным кодом, не требующая лицензионных отчислений. Стоимость владения ограничивается затратами на инфраструктуру (серверы, хранилище) и обслуживание.

**Будем использовать в задании FAISS, потому что:**
 
- Нужны скорость и автономность. Всё происходит локально: нет сетевых задержек и данные никуда не уходят.

- Легкий вход. pip install, пять строк кода и уже можно искать по смыслам.


## 4. Выбор рекомендуемой конфигурации сервера (CPU, RAM, GPU), чтобы развернуть RAG-бота.

### 1. Базовый вариант (CPU-based, для небольших нагрузок и тестирования)

- CPU: 8-16 ядер (например, Intel Xeon E-2300 series или AMD EPYC 7002 series)
- RAM: 32-64 GB DDR4
- GPU: Отсутствует (используется CPU для LLM)
- Хранилище: 256 GB - 1 TB NVMe SSD

Подходит для небольших баз знаний, начального тестирования и невысокой нагрузки. Использование CPU для LLM позволяет снизить стоимость сервера, но может увеличить время ответа.

### 2. Продвинутый вариант (GPU-based, для высоких нагрузок и низкой задержки)

- CPU: 16-32 ядер (например, Intel Xeon Gold 6300 series или AMD EPYC 7003 series)
- RAM: 64-128 GB DDR4
- GPU: NVIDIA Tesla T4 или NVIDIA RTX A4000 (или аналоги)
- Хранилище: 512 GB - 2 TB NVMe SSD

Подойдет для больших баз знаний и высокой нагрузки. Использование GPU для LLM позволяет значительно ускорить время ответа. Цена значительно отличается от базового варианта. 

### 3. Облачный вариант (масштабируемый, для переменных нагрузок)

- CPU: Виртуальные ядра процессора (vCPU), количество которых можно масштабировать в зависимости от нагрузки.
- RAM: Оперативная память, объем которой можно масштабировать в зависимости от размера базы знаний и сложности задач.
- GPU: (Опционально) Графические процессоры, которые можно добавить для ускорения работы LLM, если это необходимо.
- Хранилище: Быстрое хранилище (например, SSD) с возможностью масштабирования объема по мере роста базы знаний.

Подходит для компаний, которые хотят избежать затрат на обслуживание собственного оборудования и иметь возможность легко масштабировать ресурсы в зависимости от нагрузки. Облачная инфраструктура обеспечивает гибкость и отказоустойчивость, а также доступ к широкому спектру сервисов для мониторинга, управления и автоматизации. 

### 4. Гибридный подход (совместить вместе варианты 2 и 3) 

Этот вариант сочетает в себе преимущества локальной и облачной инфраструктуры. Конфиденциальные данные хранятся и обрабатываются локально, что обеспечивает повышенный уровень безопасности и контроля. Неконфиденциальные данные и вычислительные задачи могут быть перенесены в облако, что обеспечивает масштабируемость и гибкость. 

Можно выбрать, когда необходимо обеспечить высокий уровень безопасности и конфиденциальности данных, но при этом хочется использовать преимущества облачных технологий. В контексте QuantumForge Software, если есть действительно чувствительные данные, которые нельзя размещать в облаке, гибридный подход будет очень разумным решением.

Недостатки гибридного подхода:

- Сложность: Гибридная инфраструктура сложнее в управлении, чем полностью локальная или облачная.
- Затраты: Гибридный подход может потребовать дополнительных затрат на оборудование, программное обеспечение и персонал.

### 5. Выбор варианта

-  LLM: Начать с локальной модели, такой как Mistral-7B-Instruct-v0.1 или Meta-Llama-3-8B-Instruct, чтобы избежать затрат на API-вызовы. В будущем можно рассмотреть использование облачных моделей, таких как OpenAI или Yandex. 
- Embedding Model: Использовать локальную модель all-MiniLM-L6-v2 для быстрого создания индекса и хорошего качества поиска.
- Vector DB: FAISS для простоты внедрения и высокой скорости поиска.
- Инфраструктура: Начать с базового варианта CPU-based сервера, а затем перейти на облачное решение для масштабируемости или на гибрид.


# Задание 2. Подготовка базы знаний

Будем работать с информацией о Гарри Поттере. 

Чтобы собрать базу файлов, нужно организовать следующую конфигурацию каталогов и файлов:

(Папка "/knowledge_base")

├── replace_all.py (Скрипт Python)

├── terms_map.json  (Файл со словарем замен)

├── original/     (Папка с оригинальными файлами, которые нужно обработать)

│  ├── file1.txt

│  ├── file2.txt

│  └── ...

└── processed/    (Папка, куда будут сохранены обработанные файлы)

Далее запускем скрипт replace_all.py


































